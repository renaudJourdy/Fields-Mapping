# Description

Monitors the health of data forwarding connections and processing pipeline. Tracks connection status, packet processing rates, error rates, and system health metrics per customer and system-wide. Detects partial failures (affecting some customers) and total failures (affecting all customers), triggering appropriate alerts to enable rapid response and investigation.

**Value Delivered:**

- Real-time visibility into data forwarding health and connection status
- Early detection of failures before customers report issues
- Ability to distinguish between partial and total failures for appropriate response
- Historical health metrics for trend analysis and capacity planning
- Integration with investigation tools for rapid troubleshooting

**System Context:**
This feature aggregates health metrics from F1.1 (Navixy Data Forwarding Integration) and provides system-wide monitoring and alerting. It integrates with F1.3 (Customer Management) to track customer-level health and with F1.5 (Data Recovery) to detect data gaps that require recovery. Health monitoring enables proactive issue detection and supports operational excellence.

---

# Business Logic / Processing Logic

## Core Rules and Priorities

**Monitoring Scope:**

- Monitor connection status per customer (active, disconnected, failed)
- Monitor packet processing rates per customer (packets received, processed, failed)
- Monitor system-wide health (total connections, total packet rate, error rates)
- Monitor processing latency

**Failure Detection Priority:**

1. Total failure detection takes precedence (all customers affected)
2. Partial failure detection for individual customers
3. Degraded performance detection (slower than normal but still processing)

**Alert Priority:**

1. Total failure alerts (highest priority - immediate response required)
2. Partial failure alerts (medium priority - investigate affected customers)
3. Degraded performance alerts (lower priority - monitor trends)

**Data Collection:**

- Continuous monitoring of metrics from F1.1
- Real-time aggregation of customer-level and system-wide metrics
- Historical storage of metrics for trend analysis
- Integration with error visualization system

## Processing Steps

### Continuous Health Monitoring

1. **Collect Connection Status**
    - Query F1.1 for connection status per customer
    - Track connection state (active, disconnected, failed, reconnecting)
    - Update connection status in real-time
    - Store connection status history
2. **Collect Packet Metrics**
    - Receive packet processing metrics from F1.1 per customer
    - Track packets received, processed successfully, skipped, failed
    - Calculate packet processing rates (per minute, per hour)
    - Track processing latency (packet receipt to transformation)
3. **Calculate Error Rates**
    - Calculate error rate per customer (failed packets / total packets)
    - Calculate system-wide error rate
    - Track error trends over time
    - Identify error patterns and spikes
4. **Aggregate System Metrics**
    - Sum customer-level metrics to system-wide totals
    - Calculate system-wide connection health percentage
    - Calculate system-wide packet processing rate
    - Calculate system-wide error rate
5. **Update Health Dashboard**
    - Update real-time health dashboard with current metrics
    - Update historical trends and charts
    - Expose metrics to investigation tools
    - Store metrics for historical analysis

### Partial Failure Detection

1. **Monitor Customer Health**
    - Track connection status per customer
    - Track packet processing rate per customer
    - Track error rate per customer
    - Compare customer metrics to expected baselines
2. **Detect Failure Conditions**
    - Connection disconnected for customer
    - Packet processing stopped for customer (no packets received in threshold time)
    - Error rate exceeds threshold for customer
    - Processing latency exceeds threshold for customer
3. **Validate Failure**
    - Confirm failure condition persists beyond grace period
    - Exclude customers intentionally suspended (via F1.3)
    - Verify failure is not transient (retry/connection recovery in progress)
4. **Trigger Partial Failure Alert**
    - Generate partial failure alert with customer details
    - Include failure type, duration, affected customer
    - Include relevant metrics and error details
    - Route alert to appropriate notification channel

### Total Failure Detection

1. **Monitor System-Wide Health**
    - Track total active connections
    - Track system-wide packet processing rate
    - Track system-wide error rate
    - Compare system metrics to expected baselines
2. **Detect Total Failure Conditions**
    - All connections disconnected (no active connections)
    - System-wide packet processing stopped (no packets received in threshold time)
    - System-wide error rate exceeds critical threshold
    - System-wide processing latency exceeds critical threshold
3. **Validate Total Failure**
    - Confirm failure condition persists beyond grace period
    - Verify failure affects all active customers (not just suspended customers)
    - Exclude planned maintenance or system shutdown scenarios
4. **Trigger Total Failure Alert**
    - Generate total failure alert with system-wide details
    - Include failure type, duration, affected customer count
    - Include system-wide metrics and error details
    - Route alert to high-priority notification channel

### Health Metrics Collection

1. **Collect from F1.1**
    - Connection status per customer
    - Packets received per customer per time period
    - Packets processed successfully per customer
    - Packets skipped/failed per customer
    - Processing latency per customer
    - Connection uptime per customer
    - Error rates per customer
2. **Aggregate Metrics**
    - Combine customer-level metrics into system-wide totals (e.g., sum all packets received across all customers)
    - Calculate system-wide averages (e.g., average processing latency across all customers)
    - Calculate error rates as percentages (e.g., failed packets / total packets)
    - Prepare metrics for monitoring dashboard display
3. **Store Metrics**
    - Store real-time metrics in monitoring database
    - Store historical metrics for trend analysis
    - Store aggregated metrics per time period (hourly, daily)
    - Maintain metrics retention policy
4. **Expose Metrics**
    - Expose metrics via monitoring dashboard
    - Expose metrics via API for investigation tools
    - Expose metrics for alerting rules evaluation
    - Expose metrics for reporting and analysis

## Edge Cases and Error Handling

**Transient Connection Issues:**

- **Condition:** Connection briefly disconnects but reconnects quickly
- **Handling:** Do not trigger alert if reconnection occurs within grace period
- **Reason:** Avoid alert spam for transient network issues

**Customer Suspended:**

- **Condition:** Customer intentionally suspended via F1.3
- **Handling:** Exclude suspended customers from failure detection
- **Reason:** Suspended customers are expected to have no connection

**New Customer Without Baseline:**

- **Condition:** New customer added to data forwarding has no historical metrics to establish normal behavior
- **Handling:** Use system-wide average metrics (from all customers) or predefined default thresholds to detect failures for the new customer
- **Reason:** New customers need monitoring immediately, even before enough data exists to know their normal packet rate or error patterns

**Metrics Collection Failure:**

- **Condition:** Unable to collect metrics from F1.1
- **Handling:** Log error, attempt retry, mark metrics as unavailable
- **Reason:** Monitoring system failure should not block data forwarding

**Alert System Failure:**

- **Condition:** Alert system unavailable when failure detected
- **Handling:** Queue alerts for delivery when system recovers, log alert generation
- **Reason:** Ensure alerts are not lost during alert system outages

**Partial Failure During Total Failure:**

- **Condition:** Total failure occurs while partial failures are active
- **Handling:** Suppress partial failure alerts, focus on total failure resolution
- **Reason:** Avoid alert noise during system-wide incidents

**Grace Period Expiry:**

- **Condition:** Failure condition detected but resolves before grace period expires
- **Handling:** Do not trigger alert, log as transient issue
- **Reason:** Avoid false positives for self-resolving issues

## Monitoring Metrics

**Key Metrics Tracked:**

| Metric | Description | Collection Point | Aggregation |
| --- | --- | --- | --- |
| Connection Status | Connection state per customer (active, disconnected, failed) | F1.1 | Per customer, system-wide count |
| Active Connections | Count of active connections | F1.1 | System-wide total |
| Packets Received | Packets received per customer per time period | F1.1 | Per customer, system-wide sum |
| Packets Processed | Packets processed successfully per customer | F1.1 | Per customer, system-wide sum |
| Packets Failed | Packets failed/skipped per customer | F1.1 | Per customer, system-wide sum |
| Processing Rate | Packets processed per minute/hour per customer | Calculated | Per customer, system-wide average |
| Error Rate | Percentage of packets failed per customer | Calculated | Per customer, system-wide average |
| Processing Latency | Time from packet receipt to transformation | F1.1 | Per customer, system-wide average |
| Connection Uptime | Percentage of time connection active per customer | Calculated | Per customer, system-wide average |

**Metric Aggregation:**

- Metrics aggregated per time period (minute, hour, day)
- Metrics tracked per customer for individual analysis
- Metrics aggregated system-wide for overall health
- Metrics stored for historical trend analysis

**Metric Retention:**

- Real-time metrics: 24 hours
- Hourly aggregates: 30 days
- Daily aggregates: 1 year
- Failure events: Permanent

## Alerting Rules

**Alert Conditions:**

| Alert Type | Condition | Threshold | Severity | Notification Channel |
| --- | --- | --- | --- | --- |
| Total Failure - No Connections | All active connections disconnected | 0 active connections for > 5 minutes | Critical | High-priority channel |
| Total Failure - No Packets | System-wide packet processing stopped | 0 packets received for > 5 minutes | Critical | High-priority channel |
| Total Failure - High Error Rate | System-wide error rate exceeds threshold | Error rate > 50% for > 10 minutes | Critical | High-priority channel |
| Partial Failure - Connection Lost | Customer connection disconnected | Connection disconnected for > 60 minutes | High | Standard channel |
| Partial Failure - No Packets | Customer packet processing stopped | 0 packets received for customer for > 60 minutes | High | Standard channel |
| Partial Failure - High Error Rate | Customer error rate exceeds threshold | Error rate > 25% for customer for > 60 minutes | Medium | Standard channel |
| Degraded Performance - High Latency | Processing latency exceeds threshold | Latency > 30 seconds for > 15 minutes | Low | Standard channel |
| Degraded Performance - Low Rate | Processing rate below baseline | Rate < 50% of baseline for > 30 minutes | Low | Standard channel |

**Alert Severity Levels:**

| Severity | Description | Response Time | Escalation |
| --- | --- | --- | --- |
| Critical | Total system failure affecting all customers | Immediate | Escalate if not resolved in 15 minutes |
| High | Partial failure affecting specific customers | Within 1 hour | Escalate if not resolved in 4 hours |
| Medium | Degraded performance or elevated error rates | Within 4 hours | Escalate if not resolved in 24 hours |
| Low | Performance degradation or minor issues | Within 24 hours | Monitor trends |

**Alert Notification:**

- **No Email/Teams Spam:** Alerts should NOT trigger email or Microsoft Teams notifications for frequent errors (e.g., every second)
- **Alert Aggregation:** Aggregate similar alerts to avoid notification spam
- **Alert Deduplication:** Suppress duplicate alerts for same failure condition
- **Alert Routing:** Route alerts to appropriate channels based on severity and type

**Alert Content:**

- Failure type and severity
- Affected customers (for partial failures) or system-wide (for total failures)
- Failure duration and detection time
- Relevant metrics and error details
- Investigation links and tools

## Investigation Workflows

**Investigation Triggers:**

- Partial failure alert received
- Total failure alert received
- Customer reports missing telemetry data
- Degraded performance detected
- Manual investigation request

**Investigation Steps:**

1. **Review Alert Details**
    - Review alert type, severity, and affected customers
    - Review failure duration and detection time
    - Review relevant metrics and error details
2. **Check Connection Status**
    - Query F1.1 for connection status of affected customers
    - Review connection history and state changes
    - Check for connection establishment/closure logs
3. **Review Packet Metrics**
    - Query packet processing metrics for affected customers
    - Review packet receipt history
    - Check for processing errors or failures
4. **Analyze Error Logs**
    - Query error logs from F1.1 for affected customers
    - Review error patterns and trends
    - Identify root cause of failures
5. **Check System Health**
    - Review system-wide health metrics
    - Check for system-wide issues or resource constraints
    - Review processing latency
6. **Validate Customer Configuration**
    - Query F1.3 for customer state (active, suspended, removed)
    - Verify customer configuration is correct
    - Check for recent customer management operations
7. **Investigate Root Cause**
    - Correlate failure with recent system changes
    - Check for external service issues (Navixy)
    - Review infrastructure and resource utilization
8. **Determine Resolution**
    - Identify resolution steps based on root cause
    - Execute resolution (may involve F1.1, F1.3, or other systems)
    - Verify resolution and monitor recovery

**Data Sources:**

- F1.1 connection status and metrics
- F1.1 error logs and processing history
- F1.3 customer state and configuration
- Health monitoring metrics and alerts
- System logs and infrastructure metrics

**Investigation Tools:**

- Health monitoring dashboard
- Connection status dashboard
- Error log viewer
- Metrics query interface
- Customer management interface

## Audit Trail Logic

**Events Logged:**

| Event | Logged Information | Retention |
| --- | --- | --- |
| Partial Failure Detected | Customer reference, failure type, timestamp, metrics | Permanent |
| Total Failure Detected | Failure type, timestamp, affected customer count, metrics | Permanent |
| Alert Generated | Alert type, severity, customer reference, timestamp | Permanent |
| Metrics Collected | Customer reference, metrics, timestamp | 1 year |
| Investigation Started | Customer reference, investigator, timestamp | Permanent |
| Investigation Completed | Customer reference, root cause, resolution, timestamp | Permanent |

**Log Format:**

- Structured logging with customer reference, event type, timestamp, metrics, error details
- Logs queryable by customer reference, event type, date range, severity
- Logs integrated with investigation tools

**Log Querying:**

- Query by customer reference to view all health events for specific customer
- Query by event type to view all failures, alerts, etc.
- Query by date range for historical analysis
- Query by severity for critical issue analysis

---

# Acceptance Criteria

## AC1 - Monitor Connection Status Per Customer

**Given** customers are active for data forwarding

**When** health monitoring is running

**Then** connection status is collected from F1.1 per customer, status is updated in real-time, and status history is stored for analysis

## AC2 - Monitor Packet Processing Metrics

**Given** packets are being processed by F1.1

**When** health monitoring is running

**Then** packet metrics (received, processed, failed) are collected per customer, metrics are aggregated system-wide, and metrics are stored for historical analysis

## AC3 - Detect Partial Failure - Connection Lost

**Given** a customer connection is disconnected

**When** connection remains disconnected for > 5 minutes

**Then** partial failure is detected, alert is generated with customer details, and failure is logged in audit trail

## AC4 - Detect Partial Failure - No Packets

**Given** a customer has no packets received

**When** no packets received for > 60 minutes

**Then** partial failure is detected, alert is generated with customer details, and failure is logged in audit trail

## AC5 - Detect Total Failure - No Connections

**Given** all active customer connections are disconnected

**When** no active connections for > 60 minutes

**Then** total failure is detected, critical alert is generated, and failure is logged in audit trail

## AC6 - Detect Total Failure - No Packets

**Given** system-wide packet processing has stopped

**When** no packets received system-wide for > 60 minutes

**Then** total failure is detected, critical alert is generated, and failure is logged in audit trail

## AC7 - Handle Transient Connection Issues

**Given** a customer connection briefly disconnects but reconnects

**When** reconnection occurs within grace period (< 60 minutes)

**Then** no alert is generated, transient issue is logged, and monitoring continues normally

## AC8 - Exclude Suspended Customers from Failure Detection

**Given** a customer is suspended via F1.3

**When** health monitoring evaluates failure conditions

**Then** suspended customer is excluded from failure detection, no alerts generated for suspended customer, and monitoring continues for active customers

## AC9 - Aggregate System-Wide Metrics

**Given** customer-level metrics are collected

**When** health monitoring aggregates metrics

**Then** system-wide totals and averages are calculated, metrics are stored, and metrics are exposed via dashboard and API

## AC10 - Generate Alert with Appropriate Severity

**Given** a failure condition is detected

**When** alert is generated

**Then** alert includes failure type, severity, affected customers, metrics, and investigation links, alert is routed to appropriate notification channel, and alert is logged in audit trail

## AC11 - Provide Investigation Tools Access

**Given** a failure is detected

**When** investigation is needed

**Then** investigation tools have access to connection status, packet metrics, error logs, customer configuration, and health metrics for root cause analysis

## AC12 - Integrate with F1.1 Metrics Collection

**Given** F1.1 is processing packets

**When** health monitoring collects metrics

**Then** metrics are collected from F1.1 successfully, metrics are aggregated and stored, and metrics are available for monitoring and alerting

## AC13 - Avoid Alert Spam

**Given** errors occur frequently (e.g., every second)

**When** health monitoring evaluates alert conditions

**Then** alerts are aggregated and deduplicated, no email/Teams notifications are sent for frequent errors, and alerts are available via visualization system

---

# Related Documentation

- E1 - Provider Ingestion & Data Forwarding
- F1.1 - Navixy Data Forwarding Integration
- F1.3 - Customer Management for Data Forwarding
- F1.5 - Data Recovery System

---

# Non-Goals

**Out of Scope:**

- Data recovery procedures (handled by F1.5)
- Customer management operations (handled by F1.3)
- Packet processing details (handled by F1.1)
- Infrastructure monitoring (handled by separate infrastructure monitoring system)

**Boundaries:**

- This feature monitors and alerts on health, but does not perform recovery actions
- Connection management details are handled by F1.1
- Customer lifecycle management is handled by F1.3
- Data recovery procedures are handled by F1.5
- Alert delivery mechanisms are handled by alerting system (not email/Teams spam)