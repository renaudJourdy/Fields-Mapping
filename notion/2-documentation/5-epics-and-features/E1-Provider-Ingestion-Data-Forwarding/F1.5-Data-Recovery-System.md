# Description

Detects gaps in telemetry data and recovers missing packets by fetching data from Navixy API. Enables manual recovery triggers and automatic gap detection to ensure complete telemetry history. Transforms API data into provider-agnostic JSON format and processes recovered data through the same YAML configuration execution workflow as real-time data to maintain data integrity and consistency.

**Value Delivered:**

- Complete telemetry history by recovering missing data during connection failures or processing issues
- Manual recovery capability for investigating and fixing data gaps
- Automatic gap detection to identify missing data proactively
- Data integrity validation to ensure recovered data matches expected format
- Seamless integration with normal processing workflow

**System Context:**

This feature detects data gaps that may occur when F1.1 (Navixy Data Forwarding Integration) experiences connection failures or processing issues. It integrates with F1.1 to identify gaps and with F1.4 (Health Monitoring) to detect when recovery is needed. Recovered data is processed through the same transformation pipeline as real-time data to ensure consistency.

---

# Business Logic / Processing Logic

## Core Rules and Priorities

**Gap Detection Priority:**

1. Automatic gap detection runs continuously to identify missing data
2. Manual recovery triggers take precedence for immediate recovery needs
3. Large gaps are prioritized over small gaps for recovery efficiency

**Recovery Workflow:**

- Transform API data to provider-agnostic JSON format (same format as real-time raw packets are transformed to)
- Process JSON through YAML configuration execution (same workflow as real-time data)
- Ensure recovered data follows identical transformation pipeline as real-time data for consistency

**Data Integrity Rules:**

- Recovered data must be transformed to provider-agnostic JSON format (same as real-time data)
- Recovered data must be validated before YAML configuration execution
- Duplicate row records must be detected and prevented (idempotency)
- Recovered data must be processed through same YAML configuration execution as real-time data
- All packets within gap boundaries must be fetched and transformed (no partial recovery)

**Recovery Scope:**

- Recover data for specific gateways/assets
- Recover data for specific time ranges
- Recover data for specific customers
- Support bulk recovery for multiple gateways/assets

## Processing Steps

### Automatic Gap Detection

1. **Find Gap Boundaries**

   - Query F1.1 for last packet received timestamp per gateway (gap end)
   - Query F1.1 for last packet loss timestamp per gateway (gap start)
   - Identify time range between last packet loss and recent packet received
   - If no recent packet received, use current time as gap end boundary

2. **Determine Recovery Date Range**

   - Calculate recovery date range from gap start to gap end
   - If recent packet received exists: request data between last packet loss and recent packet received
   - If no recent packet received: adapt API call date range from last packet loss to current time
   - Ensure date range covers entire gap period

3. **Detect Missing Packets**

   - Calculate expected packet count based on time gap
   - Compare expected count with actual received count
   - Identify missing packets or time gaps

4. **Validate Gap**

   - Confirm gap is not due to intentional suspension (check F1.3)
   - Verify gap exceeds minimum threshold (avoid false positives)
   - Check if gap is already being recovered

5. **Trigger Recovery Workflow**

   - Create recovery request with gateway, date range (gap start to gap end), gap details
   - Queue recovery for processing
   - Log gap detection with boundaries in audit trail

### Manual Recovery Trigger

1. **Receive Recovery Request**

   - Accept manual recovery request with gateway, time range, customer
   - Validate recovery request parameters
   - Check if recovery is already in progress for same scope

2. **Validate Recovery Scope**

   - Verify gateway exists and is configured
   - Verify time range is valid and within retention period
   - Verify customer has access to requested gateway

3. **Create Recovery Job**

   - Create recovery job with parameters
   - Set recovery job status to pending
   - Log recovery job creation in audit trail

4. **Queue Recovery**

   - Add recovery job to recovery queue
   - Prioritize recovery job based on size and urgency
   - Notify recovery system of new job

### Fetch Missing Data from Navixy API

1. **Determine API Date Range**

   - Use gap boundaries from gap detection (last packet loss to recent packet received)
   - If recent packet received exists: request data between last packet loss and recent packet received
   - If no recent packet received: adapt API call date range from last packet loss to current time
   - Ensure date range covers entire gap period for complete recovery

2. **Prepare API Request**

   - Construct Navixy API request with gateway and calculated date range
   - Include required authentication and credentials
   - Format request according to Navixy API specification
   - Use cURL command with provider fields extracted from YAML configuration (see Developer Documentation Requirements)

3. **Execute API Call**

   - Call Navixy API to fetch telemetry data for date range
   - Handle API rate limiting and retries
   - Handle API errors and connection failures

4. **Receive API Response**

   - Receive telemetry data from Navixy API
   - Validate API response format
   - Extract telemetry records from response

5. **Validate API Data**

   - Verify data covers requested date range
   - Verify data format matches expected structure
   - Check for data completeness and quality
   - Ensure all packets within date range are fetched

### Transform API Data to Agnostic Format

1. **Convert API Data to JSON**

   - Transform Navixy API response format to provider-agnostic JSON format
   - Use same transformation logic as real-time raw packets (from F1.1)
   - Preserve all telemetry fields and metadata in JSON structure

2. **Create Agnostic JSON Data**

   - Generate provider-agnostic JSON from converted API data
   - Maintain data sequence and timestamps
   - Include metadata (gateway, timestamp, source) in JSON structure

3. **Validate JSON Format**

   - Verify JSON format matches provider-agnostic format (same as real-time data)
   - Verify JSON structure is complete and valid
   - Verify timestamps and required fields are present

4. **Store Agnostic JSON Data**

   - Store provider-agnostic JSON data in recovery storage
   - Mark data as recovered (not real-time)
   - Prepare JSON data for YAML configuration execution

### Execute YAML Configuration on Recovered Data

1. **Load Recovered JSON Data**

   - Retrieve recovered provider-agnostic JSON data
   - Validate JSON format and integrity
   - Prepare JSON data for YAML configuration execution

2. **Execute YAML Configuration**

   - Execute same YAML configuration file as used for real-time data
   - Apply same transformation rules and field mappings
   - Process JSON through transformation pipeline

3. **Handle Duplicate Detection**

   - Check if data already exists in storage (by timestamp, gateway, and unique identifiers)
   - Skip duplicate row records to avoid data duplication
   - Process only new/missing data that doesn't exist in storage
   - Ensure idempotency: same recovery job can be retried without creating duplicates

4. **Store Transformed Data**

   - Store transformed Fleeti telemetry in storage tiers
   - Update storage with recovered data
   - Mark data as recovered in metadata

### Validate Recovered Data Integrity

1. **Compare Data Coverage**

   - Compare recovered data with expected time range
   - Verify all expected packets are recovered
   - Identify any remaining gaps

2. **Validate Data Quality**

   - Verify recovered data matches expected format
   - Verify data fields are complete and valid
   - Check for data anomalies or errors

3. **Validate Transformation**

   - Verify transformed data matches real-time data format
   - Verify transformation rules applied correctly
   - Check for transformation errors or inconsistencies

4. **Report Recovery Results**

   - Generate recovery summary with statistics
   - Report recovered packet count, time range, success rate
   - Log recovery completion in audit trail

## Edge Cases and Error Handling

**Navixy API Unavailable:**

- **Condition:** Navixy API is unavailable when fetching missing data
- **Handling:** Retry with exponential backoff, queue recovery for later, log error
- **Reason:** Temporary API unavailability should not block recovery permanently

**API Rate Limiting:**

- **Condition:** Navixy API rate limits exceeded during recovery
- **Handling:** Throttle API requests, queue remaining requests, resume when rate limit resets
- **Reason:** Respect API rate limits to avoid service disruption

**Partial Data Recovery:**

- **Condition:** Only some missing packets are recovered from API
- **Handling:** Process recovered packets, log remaining gaps, allow additional recovery attempts
- **Reason:** Partial recovery is better than no recovery, remaining gaps can be recovered later

**Duplicate Packets:**

- **Condition:** Recovered packets already exist in storage
- **Handling:** Skip duplicate packets, log duplicates, continue processing new packets
- **Reason:** Prevent duplicate data in storage, maintain data integrity

**Invalid API Data Format:**

- **Condition:** Navixy API returns data in unexpected format
- **Handling:** Log error, skip invalid data, attempt to recover remaining valid data
- **Reason:** Invalid data should not block recovery of valid data

**Recovery Timeout:**

- **Condition:** Recovery job takes too long to complete
- **Handling:** Log timeout, mark job as failed, allow retry or manual intervention
- **Reason:** Long-running recoveries may indicate issues that need attention

**Large Gap Recovery:**

- **Condition:** Very large time gaps require recovery (days/weeks)
- **Handling:** Split large gaps into smaller chunks, process chunks sequentially, monitor progress
- **Reason:** Large recoveries may take significant time and resources

**Recovery Job Conflicts:**

- **Condition:** Multiple recovery jobs requested for same gateway/time range
- **Handling:** Merge or prioritize recovery jobs, prevent duplicate recovery
- **Reason:** Avoid duplicate recovery work and resource waste

## Recovery Procedures

**Recovery Triggers:**

- Automatic gap detection identifies missing data
- Manual recovery request from operator
- Health monitoring detects data gaps (F1.4 integration)
- Customer reports missing telemetry data

**Recovery Steps:**

1. **Identify Gap Boundaries**

   - Find last packet loss received timestamp (gap start)
   - Find recent packet received timestamp (gap end)
   - If no recent packet received, use current time as gap end
   - Determine recovery date range between gap boundaries
   - Verify gap is valid (not due to suspension)
   - Check if recovery already in progress

2. **Fetch Missing Data**

   - Determine API date range: between last packet loss and recent packet received (or current time if no recent packet)
   - Call Navixy API to fetch all telemetry data within date range
   - Ensure all packets within date range are fetched
   - Handle API errors and retries
   - Validate API response

3. **Transform to Agnostic Format**

   - Transform all fetched API data to provider-agnostic JSON format
   - Create JSON data matching real-time data format
   - Validate JSON format for all records

4. **Execute YAML Configuration**

   - Execute YAML configuration on all recovered JSON data
   - Transform all JSON records to Fleeti telemetry format
   - Detect and skip duplicate row records (idempotency)
   - Store only new/missing transformed data

5. **Validate Recovery**

   - Verify recovered data covers gap
   - Validate data quality and integrity
   - Check for remaining gaps

6. **Report Results**

   - Generate recovery summary
   - Log recovery completion
   - Update recovery status

**Recovery Validation:**

- Verify recovered data covers requested time range
- Verify recovered packet count matches expected count
- Verify data quality and format correctness
- Verify transformation applied correctly
- Check for duplicate packets

**Recovery Rollback:**

- Recovery cannot be rolled back (data is additive)
- Invalid recovered data can be marked as invalid
- Recovery jobs can be cancelled before completion
- Failed recovery jobs can be retried

## Investigation Workflows

**Investigation Triggers:**

- Gap detected but recovery fails
- Recovered data quality issues
- Recovery job timeout or failure
- Customer reports missing data after recovery

**Investigation Steps:**

1. **Review Gap Details**

   - Query gap detection logs for gap details
   - Review gap size, time range, gateway
   - Check gap detection accuracy

2. **Review Recovery Job**

   - Query recovery job logs for job details
   - Review recovery steps and results
   - Check for errors or failures

3. **Review API Calls**

   - Query Navixy API call logs
   - Review API requests and responses
   - Check for API errors or rate limiting

4. **Review Recovered Data**

   - Query recovered data in storage
   - Review data quality and completeness
   - Check for data anomalies

5. **Validate Recovery Process**

   - Verify recovery workflow executed correctly
   - Check for processing errors
   - Verify data transformation applied

6. **Identify Root Cause**

   - Correlate recovery failure with system events
   - Check for Navixy API issues
   - Review connection and processing history

7. **Determine Resolution**

   - Identify resolution steps based on root cause
   - Retry recovery if appropriate
   - Report issues to relevant teams

**Data Sources:**

- Gap detection logs (gap details, timestamps)
- Recovery job logs (job status, steps, results)
- Navixy API logs (API calls, responses, errors)
- Recovered data in storage (data quality, completeness)
- F1.1 processing logs (packet processing history)
- F1.4 health monitoring (connection status, gaps)

**Investigation Tools:**

- Gap detection query interface
- Recovery job status dashboard
- Navixy API call log viewer
- Recovered data query interface
- Recovery audit trail viewer

## Audit Trail Logic

**Events Logged:**

| Event | Logged Information | Retention |
| --- | --- | --- |
| Gap Detected | Gateway, time range, gap size, timestamp | Permanent |
| Recovery Job Created | Gateway, time range, customer, job ID, timestamp | Permanent |
| Recovery Job Started | Job ID, timestamp | Permanent |
| Recovery Job Completed | Job ID, recovered count, success rate, timestamp | Permanent |
| Recovery Job Failed | Job ID, error details, timestamp | Permanent |
| API Call Executed | Gateway, time range, API response status, timestamp | 1 year |
| Packets Recovered | Gateway, packet count, time range, timestamp | Permanent |
| Data Validation Failed | Gateway, validation errors, timestamp | Permanent |
| Duplicate Packets Detected | Gateway, duplicate count, timestamp | Permanent |

**Log Format:**

- Structured logging with gateway, time range, job ID, timestamp, status, error details
- Logs queryable by gateway, time range, job ID, date range, status
- Logs integrated with investigation tools

**Log Querying:**

- Query by gateway to view all gaps and recoveries for specific gateway
- Query by time range to view all gaps and recoveries in time period
- Query by job ID to view complete recovery job history
- Query by status to view failed or completed recoveries

## Data Quality Checks

**Quality Validation Rules:**

- Recovered data must match provider-agnostic JSON format (same as real-time data)
- Recovered data must have valid timestamps
- Recovered data must have required fields
- Recovered data must not be duplicate
- Recovered data must cover expected time range
- Recovered data must be successfully processed through YAML configuration

**Quality Metrics:**

| Metric | Description | Threshold |
| --- | --- | --- |
| Recovery Success Rate | Percentage of gaps successfully recovered | > 95% |
| Data Completeness | Percentage of expected packets recovered | > 95% |
| Data Quality | Percentage of recovered packets passing validation | > 99% |
| Duplicate Rate | Percentage of recovered packets that are duplicates | < 1% |
| Recovery Time | Time to complete recovery job | < 1 hour for typical gaps |

**Quality Thresholds:**

- Recovery success rate must exceed 95%
- Data completeness must exceed 95%
- Data quality must exceed 99%
- Duplicate rate must be less than 1%

**Quality Reporting:**

- Recovery summary reports with success rate, completeness, quality
- Quality metrics tracked per recovery job
- Quality trends tracked over time
- Quality issues reported for investigation

---

# Acceptance Criteria

## AC1 - Detect Gap in Received Data

**Given** packets are expected but not received for a gateway

**When** gap detection runs

**Then** last packet loss received timestamp is identified (gap start), recent packet received timestamp is identified (gap end, or current time if none), recovery date range is determined between gap boundaries, gap is logged in audit trail with boundaries, and recovery workflow is triggered

## AC2 - Manual Recovery Trigger

**Given** operator requests manual recovery for gateway and time range

**When** manual recovery request is submitted

**Then** recovery job is created with parameters, recovery job is queued for processing, and recovery job creation is logged in audit trail

## AC3 - Fetch Missing Data from Navixy API

**Given** recovery job is created with gap boundaries (last packet loss to recent packet received, or current time if no recent packet)

**When** recovery job executes

**Then** API date range is determined from gap boundaries, Navixy API is called to fetch all telemetry data within date range, all packets within date range are fetched, API response is received and validated, and API call is logged in audit trail

## AC4 - Transform API Data to Agnostic JSON Format

**Given** telemetry data is fetched from Navixy API for entire date range

**When** all fetched data is converted to provider-agnostic JSON format

**Then** all API data records are transformed to provider-agnostic JSON format, JSON format is validated for all records, and JSON data is stored for YAML configuration execution

## AC5 - Execute YAML Configuration on Recovered Data

**Given** provider-agnostic JSON data is created from recovered API data

**When** YAML configuration is executed on recovered JSON data

**Then** same YAML configuration file is used as real-time data, JSON is transformed to Fleeti telemetry format, transformed data is stored in storage tiers, and processing is logged in audit trail

## AC6 - Handle Duplicate Data

**Given** recovered data may already exist in storage

**When** recovered data is processed

**Then** duplicate row records are detected by timestamp, gateway, and unique identifiers, duplicate records are skipped, only new/missing data is processed, and duplicates are logged in audit trail

## AC7 - Validate Recovered Data Integrity

**Given** recovered data is processed and stored

**When** recovery validation runs

**Then** recovered data is validated for completeness, quality, and format, validation results are logged, and recovery summary is generated

## AC8 - Handle Navixy API Unavailable

**Given** Navixy API is unavailable during recovery

**When** API call fails

**Then** recovery job is queued for retry, error is logged, and recovery is retried when API becomes available

## AC9 - Handle API Rate Limiting

**Given** Navixy API rate limits are exceeded during recovery

**When** API rate limit is hit

**Then** API requests are throttled, remaining requests are queued, and recovery resumes when rate limit resets

## AC10 - Handle Large Gap Recovery

**Given** very large time gap requires recovery

**When** large gap recovery is initiated

**Then** gap is split into smaller chunks, chunks are processed sequentially, progress is monitored, and recovery completes successfully

## AC11 - Report Recovery Results

**Given** recovery job completes (success or failure)

**When** recovery results are generated

**Then** recovery summary includes recovered count, time range, success rate, quality metrics, and results are logged in audit trail

## AC12 - Integrate with F1.1 Gap Detection

**Given** F1.1 detects missing packets or time gaps

**When** gap is identified

**Then** F1.5 is triggered with gap details, recovery workflow is initiated, and integration is logged

## AC13 - Integrate with F1.4 Health Monitoring

**Given** F1.4 detects data gaps requiring recovery

**When** gap is detected by health monitoring

**Then** F1.5 is notified, recovery workflow is triggered, and integration is logged

---

# Developer Documentation Requirements

## Navixy API cURL Command for Data Recovery

The development team must use the following cURL command structure to fetch missing telemetry data from Navixy API:

```bash
curl --location 'https://api.eu.navixy.com/dwh/v1/tracker/raw_data/read' \
  --header 'accept: text/csv' \
  --header 'Content-Type: application/json' \
  --data '{
    "hash": "{{navixy_hash}}",
    "tracker_id": "{{tracker_id}}",
    "from": "{{from_date}}",
    "to": "{{to_date}}",
    "columns": [
      "alt",
      "discrete_inputs",
      "discrete_outputs",
      "event_id",
      "gps_fix_type",
      "hdop",
      "heading",
      "inputs.avl_io_1",
      "inputs.avl_io_102",
      "inputs.avl_io_103",
      "inputs.avl_io_104",
      "inputs.avl_io_105",
      "inputs.avl_io_106",
      "inputs.avl_io_107",
      "inputs.avl_io_108",
      "inputs.avl_io_10808",
      "inputs.avl_io_10809",
      "inputs.avl_io_10810",
      "inputs.avl_io_10811",
      "inputs.avl_io_16",
      "inputs.avl_io_179",
      "inputs.avl_io_180",
      "inputs.avl_io_181",
      "inputs.avl_io_182",
      "inputs.avl_io_2",
      "inputs.avl_io_20",
      "inputs.avl_io_201",
      "inputs.avl_io_202",
      "inputs.avl_io_203",
      "inputs.avl_io_204",
      "inputs.avl_io_207",
      "inputs.avl_io_210",
      "inputs.avl_io_212",
      "inputs.avl_io_22",
      "inputs.avl_io_23",
      "inputs.avl_io_234",
      "inputs.avl_io_239",
      "inputs.avl_io_24",
      "inputs.avl_io_240",
      "inputs.avl_io_25",
      "inputs.avl_io_26",
      "inputs.avl_io_27",
      "inputs.avl_io_270",
      "inputs.avl_io_273",
      "inputs.avl_io_28",
      "inputs.avl_io_281",
      "inputs.avl_io_282",
      "inputs.avl_io_29",
      "inputs.avl_io_3",
      "inputs.avl_io_30",
      "inputs.avl_io_304",
      "inputs.avl_io_37",
      "inputs.avl_io_380",
      "inputs.avl_io_389",
      "inputs.avl_io_390",
      "inputs.avl_io_4",
      "inputs.avl_io_449",
      "inputs.avl_io_519",
      "inputs.avl_io_69",
      "inputs.avl_io_72",
      "inputs.avl_io_73",
      "inputs.avl_io_74",
      "inputs.avl_io_75",
      "inputs.avl_io_78",
      "inputs.avl_io_81",
      "inputs.avl_io_83",
      "inputs.avl_io_84",
      "inputs.avl_io_86",
      "inputs.avl_io_87",
      "inputs.avl_io_89",
      "inputs.ble_battery_level_1",
      "inputs.ble_battery_level_2",
      "inputs.ble_battery_level_3",
      "inputs.ble_battery_level_4",
      "inputs.ble_humidity_1",
      "inputs.ble_humidity_2",
      "inputs.ble_humidity_3",
      "inputs.ble_humidity_4",
      "inputs.ble_lls_level_1",
      "inputs.ble_lls_level_2",
      "inputs.ble_magnet_sensor_1",
      "inputs.ble_magnet_sensor_2",
      "inputs.ble_magnet_sensor_3",
      "inputs.ble_magnet_sensor_4",
      "inputs.ble_temp_sensor_1",
      "inputs.ble_temp_sensor_2",
      "inputs.ble_temp_sensor_3",
      "inputs.ble_temp_sensor_4",
      "inputs.can_consumption",
      "inputs.can_consumption_relative",
      "inputs.can_engine_hours_relative",
      "inputs.can_fuel_1",
      "inputs.can_fuel_litres",
      "inputs.can_mileage_relative",
      "inputs.ext_temp_sensor_1",
      "inputs.ext_temp_sensor_2",
      "inputs.ext_temp_sensor_3",
      "inputs.ext_temp_sensor_4",
      "inputs.humidity_1",
      "inputs.humidity_2",
      "inputs.lls_level_1",
      "inputs.lls_level_2",
      "inputs.lls_level_3",
      "inputs.lls_level_4",
      "inputs.lls_temperature_1",
      "inputs.lls_temperature_2",
      "inputs.obd_custom_fuel_litres",
      "inputs.obd_custom_odometer",
      "inputs.obd_dtc_number",
      "inputs.obd_mil_status",
      "inputs.temp_sensor",
      "lat",
      "lng",
      "pdop",
      "satellites",
      "speed",
      "states.can_engine_hours",
      "states.can_mileage",
      "states.can_speed",
      "states.hw_mileage",
      "states.moving",
      "states.obd_speed"
    ]
  }'
```

**Important Notes:**

- **Provider Fields Extraction Script**: A Python script has been created (`extract_provider_fields.py`) that automatically extracts provider fields from the latest YAML configuration file. This script ensures that only provider fields referenced in the YAML configuration are included in the API call, streamlining the provider fields and simplifying the mapping logic.

- **Script Location**: `notion/2-documentation/1-field-mappings-and-databases/4-yaml-configuration/scripts/data-recovery/extract_provider_fields.py`

- **Script Output**: The script generates a JSON file (`navixy-recovery-columns-YYYY-MM-DD.json`) in the `output/` subfolder containing the exact columns array to use in the API request.

- **Updating Provider Fields**: When new provider fields are added to the YAML configuration file, the script must be re-run to extract the new provider fields and update the API call columns array. The script automatically finds the latest YAML configuration file and generates an updated JSON file with all referenced provider fields.

- **Version-Based Validation**: The JSON includes a `yaml_config_version` field that contains the YAML configuration version. This enables the backend to check if field extraction needs to be re-executed by comparing stored version with current YAML version, avoiding unnecessary re-extraction when versions match.

- **API Endpoint**: Use the appropriate Navixy API endpoint based on region:
  - EU: `https://api.eu.navixy.com/dwh/v1/tracker/raw_data/read`
  - US: `https://api.us.navixy.com/dwh/v1/tracker/raw_data/read`
  - RU: `https://api.navixy.com/dwh/v1/tracker/raw_data/read`

- **Response Format**: The API returns CSV data with column headers in the first row and data values in subsequent rows.

- **Excluded Fields**: The following fields are excluded from the API call as they cause errors or are not available:
  - `ibutton` - Not available in recovery API
  - `msg_time` - Not available in recovery API (automatically included in response)

## Backend Implementation: Automatic Provider Field Extraction

The backend must implement automatic extraction of provider fields from the latest YAML configuration file when data recovery is needed. This ensures the API payload always includes only the provider fields referenced in the current YAML configuration, streamlining the provider fields and simplifying the mapping logic.

**Reference Script**: See `extract_provider_fields.py` script attached in Notion for complete implementation details.

### Implementation Requirements

- **YAML Configuration Source**: Load the latest YAML configuration file from the backend's configuration storage (location differs from this project structure)
- **Version-Based Validation**: Use the YAML configuration `version` field to determine if provider fields extraction needs to be re-executed
- **Dynamic Payload Generation**: Extract provider fields from YAML and generate the `columns` array dynamically for each recovery API call
- **Automatic Updates**: When new provider fields are added to the YAML configuration, they are automatically included in recovery API calls without code changes

### Version-Based Optimization

The JSON payload includes a `yaml_config_version` field that contains the version from the YAML configuration file. This enables efficient version-based checking:

- **Check Version Before Extraction**: Compare stored JSON's `yaml_config_version` with current YAML's `version` field
- **Skip Extraction if Versions Match**: If versions match, reuse existing `columns` array (no need to re-extract fields)
- **Re-extract Only When Version Changes**: Only execute field extraction when YAML version has changed
- **Store Version with Payload**: Include `yaml_config_version` in stored JSON/cache for quick comparison

**Example Workflow:**
1. Load YAML configuration file → Get `version: "1.0.6"`
2. Check stored JSON/cache → Get `yaml_config_version: "1.0.5"`
3. Versions don't match → Execute field extraction → Update JSON with new version
4. Next recovery call → Versions match → Reuse existing `columns` array

### Key Extraction Rules

The following rules must be implemented when extracting provider fields from YAML:

**Field Extraction Sources:**
- Extract from `sources[].field` and `sources[].path` entries
- Extract from `parameters.provider.navixy[]` arrays (field name lists)
- Extract from `parameters.provider.navixy` single string values
- Handle special bitmask cases: `navixy: inputs` → `"discrete_inputs"`, `navixy: outputs` → `"discrete_outputs"`

**Path to API Column Mapping:**
- Root level fields (`path: lat`, `path: lng`, etc.) → Use field name as-is: `"lat"`, `"lng"`
- Params fields (`path: params.avl_io_69`) → Map to `inputs.*` by default: `"inputs.avl_io_69"`
- Known states fields (`path: params.moving`, `path: params.can_engine_hours`) → Map to `states.*`: `"states.moving"`, `"states.can_engine_hours"`
- Field name inference: When only field name available (no path), infer format based on field name patterns

**Field Name Inference Rules:**
- `avl_io_*` → `inputs.avl_io_*`
- `can_*` → `inputs.can_*` (unless in states fields list)
- `obd_*` → `inputs.obd_*`
- `ble_*` → `inputs.ble_*`
- `ext_temp_sensor_*` → `inputs.ext_temp_sensor_*`
- `lls_*` → `inputs.lls_*`
- Known states fields → `states.*` (see STATES_FIELDS list in script)

**Core Columns:**
- Always include: `"gps_fix_type"`, `"event_id"` (even if not in YAML)

**Excluded Fields:**
- Always exclude: `"ibutton"`, `"msg_time"` (cause API errors)

**Bitmask Handling:**
- If `inputs` bitmask is referenced in YAML → Add `"discrete_inputs"` to columns
- If `outputs` bitmask is referenced in YAML → Add `"discrete_outputs"` to columns

**Output Format:**
- Sort columns alphabetically
- Remove duplicates
- Generate JSON payload with `columns` array for Navixy API request

### Payload Structure

The generated payload should follow this structure (adapt to backend workflow as needed):

```json
{
  "hash": "<navixy_hash>",
  "tracker_id": "<tracker_id>",
  "from": "<from_date>",
  "to": "<to_date>",
  "yaml_config_version": "1.0.6",
  "columns": [
    // Sorted array of provider field columns extracted from YAML
  ]
}
```

**Important Notes:**
- **Version Field**: The `yaml_config_version` field must be included to enable version-based validation
- **Backend Adaptation**: The backend may adapt this structure to fit its internal workflow (e.g., store columns separately, use different field names, store version in metadata, etc.), but the `columns` array must contain the exact API column names as specified above
- **Version Storage**: The backend can store the version separately (e.g., in cache metadata, database, etc.) as long as it can be compared with the YAML configuration version

---

# Related Documentation

- E1 - Provider Ingestion & Data Forwarding
- F1.1 - Navixy Data Forwarding Integration
- F1.4 - Data Forwarding Health Monitoring

---

# Non-Goals

**Out of Scope:**

- Real-time data forwarding (handled by F1.1)
- Health monitoring and alerting (handled by F1.4)
- Customer management (handled by F1.3)
- Data transformation details (handled by transformation pipeline)
- Storage tier management (handled by storage system)

**Boundaries:**

- This feature recovers missing data but does not prevent gaps
- Gap detection details are handled by F1.1
- Health monitoring and alerting are handled by F1.4
- Data transformation is handled by transformation pipeline
- Storage management is handled by storage system

---
